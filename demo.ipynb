{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.2.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, let's forge 3 regression problems (tasks). The first regression problem has 3 outputs; second has 2 outputs; third has 5 outputs. All tasks' samples are represented as 20D vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "T = 3 # number of tasks\n",
    "O = [3, 2, 5] # number of outputs for every task\n",
    "N = 100 # number of training samples\n",
    "D = 20 # dimension of feature vector -- assumed to be the same for all tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = [np.random.randn(N,D) for _ in range(T)]\n",
    "Y = [x.dot(np.random.randn(D,o)) for x,o in zip(X,O)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second, let's train a regression model for each task independently. Regression model is realised by a 3-layer neuron network, and the hidden layer has 10 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "H = 10\n",
    "W_input_to_hidden = [tf.Variable(tf.truncated_normal(shape=[D, H])) for _ in range(T)]\n",
    "b_input_to_hidden = [tf.Variable(tf.zeros(shape=[H])) for _ in range(T)]\n",
    "W_hidden_to_output = [tf.Variable(tf.truncated_normal(shape=[H, o])) for o in O]\n",
    "b_hidden_to_output = [tf.Variable(tf.zeros(shape=[o])) for o in O]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_placeholder = [tf.placeholder(tf.float32, shape=[None, D]) for _ in range(T)]\n",
    "Y_placeholder = [tf.placeholder(tf.float32, shape=[None, o]) for o in O]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_hat = [tf.nn.xw_plus_b(tf.nn.sigmoid(tf.nn.xw_plus_b(x,w0,b0)),w1,b1) \n",
    "         for x,w0,b0,w1,b1 in zip(X_placeholder, W_input_to_hidden, b_input_to_hidden, W_hidden_to_output, b_hidden_to_output)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MSE = [tf.reduce_mean(tf.squared_difference(y,y_hat)) for y,y_hat in zip(Y_placeholder,Y_hat)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt = tf.train.AdamOptimizer(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = opt.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feed_dict = dict(list(zip(X_placeholder,X))+list(zip(Y_placeholder,Y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.3576\n",
      "9.27657\n",
      "2.90455\n",
      "1.48431\n",
      "0.951992\n",
      "0.685612\n",
      "0.505882\n",
      "0.38454\n",
      "0.302834\n",
      "0.243541\n"
     ]
    }
   ],
   "source": [
    "for _ in range(1000):\n",
    "    train.run(feed_dict=feed_dict)\n",
    "    if _ % 100 == 0:\n",
    "        print(loss.eval(feed_dict=feed_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this example, the 'shareable' layer is the input-to-hidden layer. Let's do soft sharing using the method proposed in paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We can resue the parameters learned above for initialisation, though it is optional\n",
    "W_init = np.stack(sess.run(W_input_to_hidden))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We put the task-axis in the last position\n",
    "W_init = np.transpose(W_init, axes=[1,2,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TNRDMTL.tensor_trace_norm import TensorTraceNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we use \"TensorTraceNorm\" to get the trace norm of shareable layer's parameter (tensor)\n",
    "W_input_to_hidden = tf.Variable(tf.truncated_normal(shape=[D, H, T]))\n",
    "Trace_norm_input_to_hidden = TensorTraceNorm(W_input_to_hidden, 'LAF') # Three methods: 'LAF', 'Tucker', and 'TT'\n",
    "W_input_to_hidden = [W_input_to_hidden[:,:,i] for i in range(T)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Nothing changes for bias terms or unshared parameters\n",
    "b_input_to_hidden = [tf.Variable(tf.zeros(shape=[H])) for _ in range(T)]\n",
    "W_hidden_to_output = [tf.Variable(tf.truncated_normal(shape=[H, o])) for o in O]\n",
    "b_hidden_to_output = [tf.Variable(tf.zeros(shape=[o])) for o in O]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build the network as usual\n",
    "X_placeholder = [tf.placeholder(tf.float32, shape=[None, D]) for _ in range(T)]\n",
    "Y_placeholder = [tf.placeholder(tf.float32, shape=[None, o]) for o in O]\n",
    "\n",
    "Y_hat = [tf.nn.xw_plus_b(tf.nn.sigmoid(tf.nn.xw_plus_b(x,w0,b0)),w1,b1) \n",
    "         for x,w0,b0,w1,b1 in zip(X_placeholder, W_input_to_hidden, b_input_to_hidden, W_hidden_to_output, b_hidden_to_output)]\n",
    "\n",
    "MSE = [tf.reduce_mean(tf.squared_difference(y,y_hat)) for y,y_hat in zip(Y_placeholder,Y_hat)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here we add trace norm as part of losses\n",
    "loss = tf.reduce_mean(MSE) + 0.001 * tf.reduce_sum(Trace_norm_input_to_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "\n",
    "train = opt.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model as usual\n",
    "init_op = tf.global_variables_initializer() # Workaround: https://github.com/tensorflow/tensorflow/issues/6804\n",
    "sess.run(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feed_dict = dict(list(zip(X_placeholder,X))+list(zip(Y_placeholder,Y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.6527\n",
      "8.95926\n",
      "2.87084\n",
      "1.55215\n",
      "0.995847\n",
      "0.746464\n",
      "0.603855\n",
      "0.48874\n",
      "0.391965\n",
      "0.320133\n"
     ]
    }
   ],
   "source": [
    "for _ in range(1000):\n",
    "    train.run(feed_dict=feed_dict)\n",
    "    if _ % 100 == 0:\n",
    "        print(loss.eval(feed_dict=feed_dict))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
